{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs.registration import register\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        if np.ndim(state) and np.ndim(next_state) == 1:\n",
    "            state = np.expand_dims(state, 0)\n",
    "            next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.deque, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def pop_episode(self):\n",
    "        state, action, reward, next_state, done = zip(*[self.deque.popleft() for _ in range(len(self))])\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def reset(self):\n",
    "        [self.deque.pop() for _ in range(len(self))]\n",
    "        return\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ActorCritic, self).__init__()     \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.feature_stream = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.actor_lin = nn.Linear(hidden_dim, output_dim)\n",
    "        self.critic_lin = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def to_tensor(self, x):\n",
    "        return torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if type(state) != torch.tensor:\n",
    "            state = self.to_tensor(state)\n",
    "        \n",
    "        if state.dim() == 1:\n",
    "            state.unsqueeze_(0)\n",
    "        \n",
    "        feature = self.feature_stream(state)\n",
    "        \n",
    "        action_prob = self.actor_lin(feature)\n",
    "        action_prob = torch.softmax(action_prob, dim=-1)\n",
    "        value_function = self.critic_lin(feature).squeeze(1) # dim=1\n",
    "        return action_prob, value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, actor_critic):\n",
    "        self.actor_critic = actor_critic\n",
    "        self.gamma = 0.99\n",
    "        self.lr = 0.003\n",
    "        self.optim = torch.optim.Adam(self.actor_critic.parameters(), lr=self.lr)\n",
    "        self.replay_memory = ReplayMemory(capacity=100000)\n",
    "        \n",
    "    def run_episode(self, n_episode):\n",
    "        r_sum_ls = []\n",
    "        \n",
    "        for i in range(n_episode):\n",
    "            # reset episode\n",
    "            s = env.reset()\n",
    "            r_sum = 0\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                a_prob, adv = self.actor_critic(s) # pi(a|s)\n",
    "                a_dist = torch.distributions.Categorical(a_prob) # pi(a|s)\n",
    "                a = a_dist.sample().item() # a\n",
    "                s_new, r, done, _ = env.step(a)\n",
    "                r_sum += r\n",
    "                \n",
    "                if r_sum > 100000:\n",
    "                    print('Done, No more training!')\n",
    "                    return\n",
    "                    \n",
    "                self.replay_memory.append(s, a, r, s_new, done)\n",
    "                s = s_new\n",
    "                \n",
    "            print(r_sum, end='\\r')   \n",
    "                \n",
    "            loss = self.compute_loss()\n",
    "            self.train(loss)\n",
    "            \n",
    "            \n",
    "            r_sum_ls.append(r_sum)\n",
    "            if i % 100 == 0:\n",
    "                print('Reward Sum = %.02f'%(np.mean(r_sum_ls)))\n",
    "                r_sum_ls.clear()\n",
    "        return\n",
    "    \n",
    "    def compute_loss(self):\n",
    "        # get mini-batch from replay-memory\n",
    "        S, A, R, S_next, D = self.replay_memory.pop_episode()\n",
    "        A = torch.tensor(A, dtype=torch.long)\n",
    "        R = torch.tensor(R, dtype=torch.float)\n",
    "        \n",
    "        # compute loss and gradient descent\n",
    "        a_prob, v = actor_critic(S) # pi(a_t|s_t)z\n",
    "        a_dist = torch.distributions.Categorical(a_prob) # pi(a_t|s_t)\n",
    "        log_a_prob = a_dist.log_prob(A) # A : true actions\n",
    "        _, v_next = actor_critic(S_next)\n",
    "        \n",
    "        TD_error = R + self.gamma*v_next - v\n",
    "        actor_loss = torch.mean(-log_a_prob*TD_error.data)\n",
    "        critic_loss = torch.mean(TD_error*TD_error.data)\n",
    "        loss = actor_loss + critic_loss\n",
    "        return loss\n",
    "        \n",
    "    def train(self, loss):\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Sum = 23.00\n",
      "Reward Sum = 32.07\n",
      "Reward Sum = 40.49\n",
      "Reward Sum = 72.91\n",
      "Reward Sum = 72.39\n",
      "Reward Sum = 51.38\n",
      "Reward Sum = 71.89\n",
      "Reward Sum = 78.16\n",
      "Reward Sum = 85.42\n",
      "Reward Sum = 136.96\n",
      "Reward Sum = 237.10\n",
      "Reward Sum = 339.33\n",
      "Reward Sum = 471.34\n",
      "Reward Sum = 644.98\n",
      "Reward Sum = 1020.41\n",
      "Reward Sum = 1812.33\n",
      "Reward Sum = 4071.30\n",
      "Reward Sum = 6529.71\n",
      "Reward Sum = 5895.15\n",
      "Reward Sum = 9728.50\n",
      "Reward Sum = 5950.39\n",
      "Done, No more training!\n"
     ]
    }
   ],
   "source": [
    "input_dim = 4\n",
    "hidden_dim = 128\n",
    "out_dim = env.action_space.n\n",
    "\n",
    "actor_critic = ActorCritic(input_dim, hidden_dim, out_dim)\n",
    "fitter = Fitter(actor_critic)\n",
    "\n",
    "n_episode = 100000\n",
    "fitter.run_episode(n_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, No more training!\n",
      "10001.0\n"
     ]
    }
   ],
   "source": [
    "r_sum = 0\n",
    "s = env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    a_prob, adv = fitter.actor_critic(s) # pi(a|s)\n",
    "    a_dist = torch.distributions.Categorical(a_prob) # pi(a|s)\n",
    "    a = a_dist.sample().item() # a\n",
    "    s, r, done, _ = env.step(a)\n",
    "    r_sum += r\n",
    "\n",
    "    if r_sum > 10000:\n",
    "        print('Done, No more training!')\n",
    "        break\n",
    "        \n",
    "print(r_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
