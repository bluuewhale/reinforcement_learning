{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "    img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "    img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "    img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    a = env.action_space.sample()\n",
    "    s2, _, done, _ = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fda46d7b470>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fda46c8ab00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess(s2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.deque, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameSkipper(object):\n",
    "    # consecutive frame을 4개씩 병합하여 새로운 x를 하나씩 만드는 클래스\n",
    "    # 만들어진 x는 4개씩 concat하고 s를 만들어서 DQN의 input으로 들어감\n",
    "    def __init__(self, k_skip=4):\n",
    "        self.state_deque = deque(maxlen=k_skip) # [x1, x2, x3, x4]\n",
    "        self.frame_ls = []\n",
    "        self.k_skip = k_skip\n",
    "        \n",
    "        # 최초에는 초기 화면(env.reset)으로 채워둠\n",
    "        self.env = gym.make('BreakoutDeterministic-v4')\n",
    "        padding_state = preprocess(self.env.reset())\n",
    "        for _ in range(k_skip):\n",
    "            self.state_deque.append(padding_state)\n",
    "    \n",
    "    def preprocess(self, img):\n",
    "        # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "        img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "        img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "        img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "        return img\n",
    "    \n",
    "    def aggregate_frame(self):\n",
    "        # element-wise maximum to aggregate\n",
    "        return np.maximum.reduce(self.frame_ls)\n",
    "    \n",
    "    def append_frame(self, s):\n",
    "        self.frame_ls.append(self.preprocess(s))\n",
    "        # 4개의 프레임을 skip한 후에는, 4개를 합쳐서(element-wise maximum) x를 만들고 저장\n",
    "        if len(self) == self.k_skip:\n",
    "            x = self.aggregate_frame()\n",
    "            self.state_deque.append(x)\n",
    "            self.frame_ls = []  # frame을 다시 처음부터 저장\n",
    "            \n",
    "    def get_state(self):\n",
    "        S = np.array([self.state_deque[i] for i in range(self.k_skip)]) # S = [x1, x2, x3, x4], (4, 1, 80, 80)\n",
    "        S = np.swapaxes(S, 0, 1) # (1, 4, 80, 80)\n",
    "        return S\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def to_tensor(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.to_tensor(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        output = self.fc(conved)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior_net = DQN(in_dim=4, n_action=4)\n",
    "target_net = DQN(in_dim=4, n_action=4)\n",
    "\n",
    "behavior_net.to(behavior_net.device) # model to cuda\n",
    "target_net.to(target_net.device)     # model to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay(n_total_frame):\n",
    "    return np.max([1 - 9.0*1e-07*n_total_frame, 0.1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_batch_size(current_memory, max_memory):\n",
    "    # grow batch size from 32 to 128 as respect to memory size\n",
    "    memory_ratio = current_memory / max_memory\n",
    "    new_batch_size = int(32 + 96*memory_ratio)\n",
    "    return new_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, env, behavior_net, target_net, train_method='DQN'):\n",
    "        self.env = env\n",
    "        self.train_method = train_method\n",
    "        \n",
    "        # networks\n",
    "        self.behavior_net = behavior_net\n",
    "        self.target_net = target_net\n",
    "        self.behavior_net.to(self.behavior_net.device) # model to cuda\n",
    "        self.target_net.to(self.target_net.device)     # model to cuda\n",
    "        \n",
    "        # train parameters\n",
    "        self.gamma = 0.99 #as written in paper\n",
    "        self.criterion = nn.SmoothL1Loss() # huber loss (error-clipping)\n",
    "        self.optim = torch.optim.RMSprop(self.behavior_net.parameters(), lr=0.00025) #as written in paper\n",
    "        \n",
    "        # train details\n",
    "        self.total_frame = 0\n",
    "        self.total_episode = 0\n",
    "        self.batch_size = 32\n",
    "        self.memory_size = 1000000 # as written in paper\n",
    "        self.min_replay = 50000 # as written in the paper\n",
    "        self.replay_memory = ReplayMemory(self.memory_size)\n",
    "        self.reward_ls = []\n",
    "    \n",
    "    def train(self, max_total_frame):\n",
    "        while self.total_frame < max_total_frame :\n",
    "            self.total_episode += 1\n",
    "            \n",
    "            frame_skipper = FrameSkipper(k_skip=4) #과거 k개의 frame을 하나의 state로 합쳐주는 클래스\n",
    "            s = env.reset()  # small s represents each frame\n",
    "            n_step = 0       # each episode의 step의 횟수 \n",
    "        \n",
    "            done = False\n",
    "            while not done:\n",
    "                self.total_frame += 1\n",
    "                n_step += 1\n",
    "                e = epsilon_decay(self.total_frame)\n",
    "\n",
    "                # start of every 4 consecutive frames, choose action!                \n",
    "                if n_step%4 == 1:\n",
    "                    # S = [x1, x2, x3, x4] for neural-network input\n",
    "                    # x represents feature(element-wise max) from every 4 consecutive frames(4*s)\n",
    "                    S = frame_skipper.get_state()\n",
    "                    self.S = S\n",
    "                    r_sum = 0    # sum of reward for the following 4 frames\n",
    "                    \n",
    "                    # if it is the first frame of the game, Do Something!\n",
    "                    if n_step == 1:\n",
    "                        a = random.choice([1,2,3])\n",
    "                    \n",
    "                    # else, Choose an action by e-greedy\n",
    "                    else:\n",
    "                        if np.random.rand(1) < e :\n",
    "                            a = env.action_space.sample()\n",
    "                        else:\n",
    "                            q_pred = self.behavior_net(S)\n",
    "                            a = torch.argmax(q_pred).item()\n",
    "                \n",
    "                # repeat the same action 4 times\n",
    "                s_next, r, done, _, = env.step(a)\n",
    "                frame_skipper.append_frame(s_next)\n",
    "                r_sum += r\n",
    "\n",
    "                # end of every 4 consecutive frames\n",
    "                # genenrate new x, and update S\n",
    "                if n_step%4 == 0 :\n",
    "                    S_new = frame_skipper.get_state()\n",
    "                    self.replay_memory.append(S, a, r_sum, S_new, done)\n",
    "                \n",
    "                # no training when not enough replay\n",
    "                if len(self.replay_memory) < self.min_replay:\n",
    "                    continue\n",
    "                else:\n",
    "                    # train\n",
    "                    batch_size = grow_batch_size(len(self.replay_memory), self.memory_size)\n",
    "                    mini_batch = self.replay_memory.sample(batch_size)\n",
    "                    self.train_batch(mini_batch)                \n",
    "                # single-episode(game) is now done\n",
    "            \n",
    "            \n",
    "            # no updating target, no testing when not enough replay\n",
    "            if len(self.replay_memory) < self.min_replay:\n",
    "                continue\n",
    "\n",
    "            # update target, do test for every 10 episodes(games)\n",
    "            if self.total_episode%10 == 0:\n",
    "                print('total_frame: %s'%self.total_frame)\n",
    "                fitter.update_target()\n",
    "                reward = fitter.test()\n",
    "                self.reward_ls.append(reward)  \n",
    "                \n",
    "                \n",
    "    def train_batch(self, mini_batch):\n",
    "        batch_size = len(mini_batch)\n",
    "        S = np.array([tup[0] for tup in mini_batch]).squeeze(1) # State\n",
    "        A = np.array([tup[1] for tup in mini_batch]) # actions\n",
    "        R = np.array([tup[2] for tup in mini_batch]) # rewards\n",
    "        S_next = np.array([tup[3] for tup in mini_batch]).squeeze(1) # next_State\n",
    "        D = np.array([tup[4] for tup in mini_batch]) # dones\n",
    "        \n",
    "        q_targets = self.target_net(S) # Q-values of current state with targetDDQN\n",
    "        q_targets_next = self.target_net(S_next) # Q-values of next state from targetDDQN\n",
    "        \n",
    "        if self.train_method=='DQN':\n",
    "            for i in range(batch_size):\n",
    "                a, r, done = A[i], R[i], D[i]\n",
    "                if done:\n",
    "                    q_targets[i, a] = r\n",
    "                else:\n",
    "                    q_targets[i, a] = r + self.gamma*torch.max(q_targets_next[i])\n",
    "        \n",
    "        elif self.train_method=='DoubleDQN':\n",
    "            next_behavior_net_actions = torch.argmax(self.behavior_net(S_next), dim=1) # choose argmax actions from behaviorDDQN in S_next\n",
    "                    \n",
    "            for i in range(batch_size):\n",
    "                a, r, done = A[i], R[i], D[i]\n",
    "                next_behavior_net_action = next_behavior_net_actions[i].item() # choose argmax actions from behaviorDDQN in S_next\n",
    "                if done:\n",
    "                    q_targets[i, a] = r\n",
    "                else:\n",
    "                    q_targets[i, a] = r + self.gamma*q_targets_next[i, next_behavior_net_action]\n",
    "            \n",
    "        # 예측치(pred)와 목표치(true)\n",
    "        q_behaviors = self.behavior_net(S)\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_targets, q_behaviors)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.behavior_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        frame_skipper = FrameSkipper()\n",
    "        initial_state = self.env.reset()  # save initial state for comparison\n",
    "        s = self.env.reset()\n",
    "        e = 0.05 # e-greedy, \n",
    "        \n",
    "        n_step = 0\n",
    "        r_sum = 0\n",
    "        done=False\n",
    "        while not done:\n",
    "            # 초기 화면에서는 1,2,3 중 임의의 액션을 실행\n",
    "            # 게임이 시작된 후, 공을 놓치면, life가 깍이고 다시 초기화면으로 돌아옴 \n",
    "            if np.array_equal(s, initial_state):\n",
    "                a = random.choice([1, 2, 3])\n",
    "            else:\n",
    "                # e-greedy search, e=0.05\n",
    "                if np.random.rand(1) < e:\n",
    "                    a = self.env.action_space.sample()\n",
    "                else:\n",
    "                    S = frame_skipper.get_state()\n",
    "                    a = torch.argmax(self.behavior_net(S)).item()\n",
    "            \n",
    "            s, r, done, _ = self.env.step(a)\n",
    "            frame_skipper.append_frame(s)\n",
    "            r_sum += r\n",
    "            n_step += 1\n",
    "        print('Total Step: %s \\t Total Score: %s'%(n_step, r_sum))\n",
    "        return r_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(env, behavior_net, target_net, train_method='DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_frame: 887105\n",
      "Total Step: 151 \t Total Score: 1.0\n",
      "total_frame: 889062\n",
      "Total Step: 150 \t Total Score: 1.0\n",
      "total_frame: 890984\n",
      "Total Step: 248 \t Total Score: 3.0\n",
      "total_frame: 893001\n",
      "Total Step: 150 \t Total Score: 1.0\n",
      "total_frame: 895053\n",
      "Total Step: 265 \t Total Score: 3.0\n",
      "total_frame: 897304\n",
      "Total Step: 198 \t Total Score: 2.0\n",
      "total_frame: 899657\n",
      "Total Step: 233 \t Total Score: 3.0\n",
      "total_frame: 902245\n",
      "Total Step: 377 \t Total Score: 6.0\n",
      "total_frame: 905058\n",
      "Total Step: 373 \t Total Score: 6.0\n",
      "total_frame: 907699\n",
      "Total Step: 198 \t Total Score: 2.0\n",
      "total_frame: 910267\n",
      "Total Step: 198 \t Total Score: 2.0\n",
      "total_frame: 912606\n",
      "Total Step: 349 \t Total Score: 5.0\n",
      "total_frame: 915490\n",
      "Total Step: 218 \t Total Score: 2.0\n",
      "total_frame: 917754\n",
      "Total Step: 199 \t Total Score: 2.0\n",
      "total_frame: 920517\n",
      "Total Step: 199 \t Total Score: 2.0\n",
      "total_frame: 922811\n",
      "Total Step: 261 \t Total Score: 3.0\n",
      "total_frame: 924795\n",
      "Total Step: 170 \t Total Score: 1.0\n",
      "total_frame: 927214\n",
      "Total Step: 150 \t Total Score: 1.0\n",
      "total_frame: 929252\n",
      "Total Step: 122 \t Total Score: 0.0\n",
      "total_frame: 931631\n",
      "Total Step: 269 \t Total Score: 3.0\n",
      "total_frame: 933859\n",
      "Total Step: 153 \t Total Score: 1.0\n",
      "total_frame: 935920\n",
      "Total Step: 122 \t Total Score: 0.0\n",
      "total_frame: 938640\n",
      "Total Step: 253 \t Total Score: 3.0\n",
      "total_frame: 941013\n",
      "Total Step: 200 \t Total Score: 2.0\n",
      "total_frame: 943049\n",
      "Total Step: 354 \t Total Score: 9.0\n",
      "total_frame: 945628\n",
      "Total Step: 150 \t Total Score: 1.0\n",
      "total_frame: 947755\n",
      "Total Step: 247 \t Total Score: 3.0\n",
      "total_frame: 950030\n",
      "Total Step: 218 \t Total Score: 2.0\n",
      "total_frame: 952353\n",
      "Total Step: 218 \t Total Score: 2.0\n",
      "total_frame: 954994\n",
      "Total Step: 255 \t Total Score: 3.0\n",
      "total_frame: 957643\n",
      "Total Step: 349 \t Total Score: 5.0\n",
      "total_frame: 960037\n",
      "Total Step: 129 \t Total Score: 0.0\n",
      "total_frame: 962577\n",
      "Total Step: 129 \t Total Score: 0.0\n",
      "total_frame: 964901\n",
      "Total Step: 250 \t Total Score: 3.0\n",
      "total_frame: 967492\n",
      "Total Step: 237 \t Total Score: 3.0\n",
      "total_frame: 969777\n",
      "Total Step: 281 \t Total Score: 4.0\n",
      "total_frame: 972238\n",
      "Total Step: 219 \t Total Score: 2.0\n",
      "total_frame: 975057\n",
      "Total Step: 336 \t Total Score: 5.0\n",
      "total_frame: 977543\n",
      "Total Step: 345 \t Total Score: 5.0\n",
      "total_frame: 980034\n",
      "Total Step: 397 \t Total Score: 7.0\n",
      "total_frame: 982328\n",
      "Total Step: 129 \t Total Score: 0.0\n",
      "total_frame: 984877\n",
      "Total Step: 317 \t Total Score: 5.0\n",
      "total_frame: 987364\n",
      "Total Step: 129 \t Total Score: 0.0\n",
      "total_frame: 989992\n",
      "Total Step: 295 \t Total Score: 4.0\n",
      "total_frame: 992107\n",
      "Total Step: 213 \t Total Score: 3.0\n",
      "total_frame: 994621\n",
      "Total Step: 130 \t Total Score: 0.0\n",
      "total_frame: 996550\n",
      "Total Step: 467 \t Total Score: 6.0\n",
      "total_frame: 999201\n",
      "Total Step: 181 \t Total Score: 2.0\n",
      "total_frame: 1001515\n",
      "Total Step: 341 \t Total Score: 5.0\n",
      "total_frame: 1004018\n",
      "Total Step: 374 \t Total Score: 3.0\n",
      "total_frame: 1006420\n",
      "Total Step: 150 \t Total Score: 1.0\n",
      "total_frame: 1008114\n",
      "Total Step: 363 \t Total Score: 6.0\n",
      "total_frame: 1010582\n",
      "Total Step: 253 \t Total Score: 3.0\n",
      "total_frame: 1013425\n",
      "Total Step: 517 \t Total Score: 7.0\n",
      "total_frame: 1016471\n",
      "Total Step: 397 \t Total Score: 4.0\n",
      "total_frame: 1019198\n",
      "Total Step: 253 \t Total Score: 3.0\n",
      "total_frame: 1021835\n",
      "Total Step: 281 \t Total Score: 4.0\n",
      "total_frame: 1024437\n",
      "Total Step: 321 \t Total Score: 5.0\n"
     ]
    }
   ],
   "source": [
    "max_frame = 10000000\n",
    "fitter.train(max_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how skip-frame works\n",
    "plt.imshow(fitter.S[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fda461230f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# how skip-frame works\n",
    "plt.imshow(fitter.S[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
