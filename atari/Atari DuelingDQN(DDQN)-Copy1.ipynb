{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "    img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "    img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "    img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    a = env.action_space.sample()\n",
    "    s2, _, done, _ = env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f12fac56b38>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f12fab63e80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess(s2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.deque, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameSkipper(object):\n",
    "    # consecutive frame을 4개씩 병합하여 새로운 x를 하나씩 만드는 클래스\n",
    "    # 만들어진 x는 4개씩 concat하고 s를 만들어서 DQN의 input으로 들어감\n",
    "    def __init__(self, k_skip=4):\n",
    "        self.state_deque = deque(maxlen=k_skip) # [x1, x2, x3, x4]\n",
    "        self.frame_ls = []\n",
    "        self.k_skip = k_skip\n",
    "        \n",
    "        # 최초에는 초기 화면(env.reset)으로 채워둠\n",
    "        self.env = gym.make('BreakoutDeterministic-v4')\n",
    "        padding_state = preprocess(self.env.reset())\n",
    "        for _ in range(k_skip):\n",
    "            self.state_deque.append(padding_state)\n",
    "    \n",
    "    def preprocess(self, img):\n",
    "        # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "        img = np.mean(img, axis=2).astype(np.uint8) # to gray, uint8 for low memory\n",
    "        img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "        img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "        return img\n",
    "    \n",
    "    def append_frame(self, s):\n",
    "        self.frame_ls.append(self.preprocess(s))\n",
    "        # 4개의 프레임을 skip한 후에는, 4개를 합쳐서(element-wise maximum) x를 만들고 저장\n",
    "        if len(self) == self.k_skip:\n",
    "            x = self.aggregate_frame()\n",
    "            self.state_deque.append(x)\n",
    "            self.frame_ls = []  # frame을 다시 처음부터 저장\n",
    "    \n",
    "    def aggregate_frame(self):\n",
    "        # element-wise maximum to aggregate\n",
    "        return np.maximum.reduce(self.frame_ls)\n",
    "    \n",
    "            \n",
    "    def get_state(self):\n",
    "        S = np.array([self.state_deque[i] for i in range(self.k_skip)]) # S = [x1, x2, x3, x4], (4, 1, 80, 80)\n",
    "        S = np.swapaxes(S, 0, 1) # (1, 4, 80, 80)\n",
    "        return S\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    # Dueling DQN\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        # V(s) : value-stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # A(s,a) : action-stream\n",
    "        self.action_stream = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def to_tensor(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.to_tensor(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        \n",
    "        # key of Dueling DQN : Q(s,a_i) = V(S) + A(s,a_i) - mean(A)\n",
    "        value = self.value_stream(conved) # V(s), dim=(batch_size, 1)\n",
    "        action = self.action_stream(conved) # A(s,a), dim=(batch_size, n_action)\n",
    "        \n",
    "        output = value + (action - torch.mean(action, dim=1, keepdim=True)) # V(s) + (A(s,a_i) - mean(A)), dim=(batch_size, n_action)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (value_stream): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (action_stream): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior_net = DDQN(in_dim=4, n_action=4)\n",
    "target_net = DDQN(in_dim=4, n_action=4)\n",
    "\n",
    "behavior_net.to(behavior_net.device) # model to cuda\n",
    "target_net.to(target_net.device)     # model to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay(n_total_frame):\n",
    "    return np.max([1 - 9.0*1e-07*n_total_frame, 0.1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_batch_size(current_memory, max_memory):\n",
    "    # grow batch size from 32 to 128 as respect to memory size\n",
    "    memory_ratio = current_memory / max_memory\n",
    "    new_batch_size = int(32 + 96*memory_ratio)\n",
    "    return new_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, env, behavior_net, target_net, train_method='DQN'):\n",
    "        self.env = env\n",
    "        self.train_method = train_method\n",
    "        \n",
    "        # networks\n",
    "        self.behavior_net = behavior_net\n",
    "        self.target_net = target_net\n",
    "        self.behavior_net.to(self.behavior_net.device) # model to cuda\n",
    "        self.target_net.to(self.target_net.device)     # model to cuda\n",
    "        \n",
    "        # train parameters\n",
    "        self.gamma = 0.99 #as written in paper\n",
    "        self.criterion = nn.SmoothL1Loss() # huber loss (error-clipping)\n",
    "        self.optim = torch.optim.RMSprop(self.behavior_net.parameters(), lr=0.00025) #as written in paper\n",
    "        \n",
    "        # train details\n",
    "        self.total_frame = 0\n",
    "        self.total_episode = 0\n",
    "        self.batch_size = 32\n",
    "        self.memory_size = 1000000 # as written in paper\n",
    "        self.min_replay = 50000 # as written in the paper\n",
    "        self.replay_memory = ReplayMemory(self.memory_size)\n",
    "        self.reward_ls = []\n",
    "    \n",
    "    def train(self, max_total_frame):\n",
    "        while self.total_frame < max_total_frame :\n",
    "            self.total_episode += 1\n",
    "            \n",
    "            frame_skipper = FrameSkipper(k_skip=4) #과거 k개의 frame을 하나의 state로 합쳐주는 클래스\n",
    "            s = env.reset()  # small s represents each frame\n",
    "            n_step = 0       # each episode의 step의 횟수 \n",
    "            \n",
    "            # re-calculate batch_size\n",
    "            e = epsilon_decay(self.total_frame)\n",
    "            batch_size = grow_batch_size(len(self.replay_memory), self.memory_size)\n",
    "                    \n",
    "            done = False\n",
    "            while not done:\n",
    "                self.total_frame += 1\n",
    "                n_step += 1\n",
    "                \n",
    "                # update target_net every 10,000 steps\n",
    "                if n_step%10000 == 0 :\n",
    "                    self.update_target()\n",
    "                \n",
    "                # start of every 4 consecutive frames, choose action!                \n",
    "                if n_step%4 == 1:\n",
    "                    # S = [x1, x2, x3, x4] for neural-network input\n",
    "                    # x represents feature(element-wise max) from every 4 consecutive frames(4*s)\n",
    "                    S = frame_skipper.get_state()\n",
    "                    self.S = S\n",
    "                    r_sum = 0    # sum of reward for the following 4 frames\n",
    "                    \n",
    "                    # if it is the first frame of the game, Do Something!\n",
    "                    if n_step == 1:\n",
    "                        a = random.choice([1,2,3])\n",
    "                    \n",
    "                    # else, Choose an action by e-greedy\n",
    "                    else:\n",
    "                        if np.random.rand(1) < e :\n",
    "                            a = env.action_space.sample()\n",
    "                        else:\n",
    "                            q_pred = self.behavior_net(S)\n",
    "                            a = torch.argmax(q_pred).item()\n",
    "                \n",
    "                # repeat the same action 4 times\n",
    "                s_next, r, done, _, = env.step(a)\n",
    "                frame_skipper.append_frame(s_next)\n",
    "                r_sum += r\n",
    "\n",
    "                # end of every 4 consecutive frames\n",
    "                # genenrate new x, and update S\n",
    "                if n_step%4 == 0 :\n",
    "                    S_new = frame_skipper.get_state()\n",
    "                    self.replay_memory.append(S, a, r_sum, S_new, done)\n",
    "                \n",
    "                # no training when not enough replay\n",
    "                if len(self.replay_memory) < self.min_replay:\n",
    "                    continue\n",
    "                else:\n",
    "                    # train\n",
    "                    mini_batch = self.replay_memory.sample(batch_size)\n",
    "                    self.train_batch(mini_batch)                \n",
    "                # single-episode(game) is now done\n",
    "            \n",
    "            \n",
    "            # no updating target, no testing when not enough replay\n",
    "            if len(self.replay_memory) < self.min_replay:\n",
    "                continue\n",
    "\n",
    "            # update target, do test for every 10 episodes(games)\n",
    "            if self.total_episode%10 == 0:\n",
    "                print('total_frame: %s'%self.total_frame)\n",
    "                reward = fitter.test()\n",
    "                self.reward_ls.append(reward)  \n",
    "                \n",
    "                \n",
    "    def train_batch(self, mini_batch):\n",
    "        batch_size = len(mini_batch)\n",
    "        S = np.array([tup[0] for tup in mini_batch]).squeeze(1) # State\n",
    "        A = np.array([tup[1] for tup in mini_batch]) # actions\n",
    "        R = np.array([tup[2] for tup in mini_batch]) # rewards\n",
    "        S_next = np.array([tup[3] for tup in mini_batch]).squeeze(1) # next_State\n",
    "        D = np.array([tup[4] for tup in mini_batch]) # dones\n",
    "        \n",
    "        q_targets = self.target_net(S) # Q-values of current state with targetDDQN\n",
    "        q_targets_next = self.target_net(S_next) # Q-values of next state from targetDDQN\n",
    "        \n",
    "        if self.train_method=='DQN':\n",
    "            for i in range(batch_size):\n",
    "                a, r, done = A[i], R[i], D[i]\n",
    "                if done:\n",
    "                    q_targets[i, a] = r\n",
    "                else:\n",
    "                    q_targets[i, a] = r + self.gamma*torch.max(q_targets_next[i])\n",
    "        \n",
    "        elif self.train_method=='DoubleDQN':\n",
    "            next_behavior_net_actions = torch.argmax(self.behavior_net(S_next), dim=1) # choose argmax actions from behaviorDDQN in S_next\n",
    "                    \n",
    "            for i in range(batch_size):\n",
    "                a, r, done = A[i], R[i], D[i]\n",
    "                next_behavior_net_action = next_behavior_net_actions[i].item() # choose argmax actions from behaviorDDQN in S_next\n",
    "                if done:\n",
    "                    q_targets[i, a] = r\n",
    "                else:\n",
    "                    q_targets[i, a] = r + self.gamma*q_targets_next[i, next_behavior_net_action]\n",
    "            \n",
    "        # 예측치(pred)와 목표치(true)\n",
    "        q_behaviors = self.behavior_net(S)\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_targets, q_behaviors)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.behavior_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        frame_skipper = FrameSkipper()\n",
    "        initial_state = self.env.reset()  # save initial state for comparison\n",
    "        s = self.env.reset()\n",
    "        e = 0.05 # e-greedy, \n",
    "        \n",
    "        n_step = 0\n",
    "        r_sum = 0\n",
    "        done=False\n",
    "        while not done:\n",
    "            # 초기 화면에서는 1,2,3 중 임의의 액션을 실행\n",
    "            # 게임이 시작된 후, 공을 놓치면, life가 깍이고 다시 초기화면으로 돌아옴 \n",
    "            if np.array_equal(s, initial_state):\n",
    "                a = random.choice([1, 2, 3])\n",
    "            else:\n",
    "                # e-greedy search, e=0.05\n",
    "                if np.random.rand(1) < e:\n",
    "                    a = self.env.action_space.sample()\n",
    "                else:\n",
    "                    S = frame_skipper.get_state()\n",
    "                    a = torch.argmax(self.behavior_net(S)).item()\n",
    "            \n",
    "            s, r, done, _ = self.env.step(a)\n",
    "            frame_skipper.append_frame(s)\n",
    "            r_sum += r\n",
    "            n_step += 1\n",
    "            \n",
    "        print('Total Step: %s \\t Total Score: %s'%(n_step, r_sum))\n",
    "        return r_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(env, behavior_net, target_net, train_method='DoubleDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_frame: 25443\n",
      "Total Step: 351 \t Total Score: 0.0\n",
      "total_frame: 27873\n",
      "Total Step: 301 \t Total Score: 1.0\n",
      "total_frame: 30289\n",
      "Total Step: 470 \t Total Score: 0.0\n",
      "total_frame: 32680\n",
      "Total Step: 540 \t Total Score: 0.0\n",
      "total_frame: 35022\n",
      "Total Step: 331 \t Total Score: 0.0\n",
      "total_frame: 37604\n",
      "Total Step: 457 \t Total Score: 0.0\n",
      "total_frame: 40185\n",
      "Total Step: 1278 \t Total Score: 0.0\n",
      "total_frame: 42758\n",
      "Total Step: 294 \t Total Score: 0.0\n",
      "total_frame: 45055\n",
      "Total Step: 299 \t Total Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "max_frame = 10000000\n",
    "fitter.train(max_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4ceb28b710>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# how skip-frame works\n",
    "plt.imshow(fitter.S[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.replay_memory = replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou: \n",
    "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
    "    https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "tree_index  0 0  0  We fill the leaves from left to right\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "        \n",
    "        # Update the leaf\n",
    "        self.update(tree_index, priority)\n",
    "        \n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "        \n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propagate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            \n",
    "            \"\"\" \n",
    "            Here we want to access the line above\n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "            \n",
    "                0\n",
    "               / \\\n",
    "              1   2\n",
    "             / \\ / \\\n",
    "            3  4 5  [6] \n",
    "            \n",
    "            If we are in leaf at index 6, we updated the priority score\n",
    "            We need then to update index 2 node\n",
    "            So tree_index = (tree_index - 1) // 2\n",
    "            tree_index = (6-1)//2\n",
    "            tree_index = 2 (because // round the result)\n",
    "            \"\"\"\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for experiences\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True: # the while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            \n",
    "            else: # downward search, always search for a higher priority node  \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "            \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SumTree(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.add(0.1, 1)\n",
    "tree.add(0.2, 2)\n",
    "tree.add(0.3, 3)\n",
    "tree.add(0.4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0.3, 0.7, 0.1, 0.2, 0.3, 0.4])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 0.2, 2)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.get_leaf(0.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0.3, 0.7, 0.1, 0.2, 0.3, 0.4])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_priority = max(tree.tree[-tree.capacity:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_, done ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree \n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        \n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        \n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "        \n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        \n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "    \n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "                                   \n",
    "            b_idx[i]= index\n",
    "            \n",
    "            experience = [data]\n",
    "            \n",
    "            memory_b.append(experience)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)\n",
    "\"\"\"\n",
    "This function will do the part\n",
    "With ϵ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability\n",
    "In [ ]:\n",
    "# This function helps us to copy one set of variables to another\n",
    "# In our case we use it when we want to copy the parameters of DQN to Target_network\n",
    "# Thanks of the very good implementation of Arthur Juliani https://github.com/awjuliani\n",
    "def update_target_graph():\n",
    "    \n",
    "    # Get the parameters of our DQNNetwork\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"DQNetwork\")\n",
    "    \n",
    "    # Get the parameters of our Target_network\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TargetNetwork\")\n",
    "\n",
    "    op_holder = []\n",
    "    \n",
    "    # Update our target_network parameters with DQNNetwork parameters\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
