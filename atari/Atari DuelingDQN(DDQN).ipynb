{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "    img = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "    img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "    img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "    return img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1224"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "done = False\n",
    "n = 0\n",
    "while not done:\n",
    "    a = random.choice([0, 1, 2, 3])\n",
    "    s, _, done, _ = env.step(a)\n",
    "    n += 1\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbd983f6ac8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbd9838e550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess(s)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random \n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.deque.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.deque, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameHistory(object):\n",
    "    # consecutive frame을 4개씩 병합하여 새로운 x를 하나씩 만드는 클래스\n",
    "    # 만들어진 x는 4개씩 concat하고 s를 만들어서 DQN의 input으로 들어감\n",
    "    def __init__(self, env, k=4):\n",
    "        self.state_deque = deque(maxlen=k) # [x1, x2, x3, x4]\n",
    "        self.frame_history_deque = deque(maxlen=k) #[s1, s2, s3, s4]\n",
    "        self.k = k\n",
    "        \n",
    "        # 최초에는 초기 화면(env.reset)으로 채워둠\n",
    "        self.env = env\n",
    "        self.initial_state = self.preprocess(self.env.reset())\n",
    "        self.reset()\n",
    "        \n",
    "    def preprocess(self, img):\n",
    "        # 저장할 때는 전처리를 거친 후 np.array 형태로 저장\n",
    "        img = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "        img = img[::2, ::2][17:97] # downsample(1/2) & to square\n",
    "        img = np.expand_dims(img, 0) # (1, 80, 80)\n",
    "        return img.astype(np.uint8)\n",
    "    \n",
    "    def append_frame(self, s):\n",
    "        # if full, aggregate and clear old frames, append new frame to history queue\n",
    "        if len(self) == self.k:\n",
    "            self.state_deque.append(self.aggregate_frame())\n",
    "            \n",
    "        self.frame_history_deque.append(self.preprocess(s))\n",
    "        return\n",
    "        \n",
    "    def aggregate_frame(self):\n",
    "        if len(self) < self.k:\n",
    "            raise ValueError('not enough frames in history, expected %s, but got %s'%(self.k, len(self)))\n",
    "        \n",
    "        # element-wise maximum to aggregate\n",
    "        return np.maximum.reduce([self.frame_history_deque.pop() for _ in range(len(self))][-k//2:])\n",
    "    \n",
    "    def get_state(self):\n",
    "        S = np.array([self.state_deque[i] for i in range(self.k)]) # S = [x1, x2, x3, x4], (4, 1, 80, 80)\n",
    "        S = np.swapaxes(S, 0, 1) # (1, 4, 80, 80)\n",
    "        return S\n",
    "    \n",
    "    def reset(self):\n",
    "        self.frame_history_deque = deque(maxlen=k)\n",
    "        self.state_deque = deque(maxlen=k)\n",
    "        \n",
    "        for _ in range(self.k):\n",
    "            self.state_deque.append(self.initial_state)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_history_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    # Dueling DQN\n",
    "    def __init__(self, in_dim, n_action):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 32, (8, 8), stride=4), nn.ReLU(), # conv 1\n",
    "            nn.Conv2d(32, 64, (4, 4), stride=2), nn.ReLU(), # conv 2\n",
    "            nn.Conv2d(64, 64, (3, 3), stride=1), nn.ReLU() # conv 3\n",
    "        )\n",
    "        # V(s) : value-stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(), # hidden layer\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # A(s,a) : action-stream\n",
    "        self.action_stream = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_action)\n",
    "        )\n",
    "        \n",
    "    def prep_for_input(self, img):\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=self.device).cuda(non_blocking=True) # to tensor\n",
    "        img /= 255                                   # normalize into 0-1\n",
    "        while img.dim() < 4 :                        # 4-dim\n",
    "            img = img.unsqueeze(0)\n",
    "        return img\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        frames = self.prep_for_input(frames) \n",
    "        conved = self.conv(frames)\n",
    "        conved = conved.view(conved.size(0), -1)\n",
    "        \n",
    "        # key of Dueling DQN : Q(s,a_i) = V(S) + A(s,a_i) - mean(A)\n",
    "        value = self.value_stream(conved) # V(s), dim=(batch_size, 1)\n",
    "        action = self.action_stream(conved) # A(s,a), dim=(batch_size, n_action)\n",
    "        \n",
    "        output = value + (action - torch.mean(action, dim=1, keepdim=True)) # V(s) + (A(s,a_i) - mean(A)), dim=(batch_size, n_action)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def weight_init(m):\n",
    "    '''\n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    '''\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        init.normal_(m.bias.data)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (value_stream): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (action_stream): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############\n",
    "k = 4 ## number of skipped frame\n",
    "#############\n",
    "\n",
    "behavior_net = DDQN(in_dim=k, n_action=4)\n",
    "target_net = DDQN(in_dim=k, n_action=4)\n",
    "\n",
    "behavior_net.to(behavior_net.device) # model to cuda\n",
    "target_net.to(target_net.device)     # model to cuda\n",
    "\n",
    "behavior_net.apply(weight_init)\n",
    "target_net.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_batch_size(current_memory, max_memory):\n",
    "    # grow batch size from 32 to 128 as respect to memory size\n",
    "    memory_ratio = current_memory / max_memory\n",
    "    new_batch_size = int(32 + 32*memory_ratio)\n",
    "    return new_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter():\n",
    "    def __init__(self, env, behavior_net, target_net, k=4, train_method='DQN'):\n",
    "        self.env = env\n",
    "        self.train_method = train_method\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # networks\n",
    "        self.behavior_net = behavior_net\n",
    "        self.target_net = target_net\n",
    "        self.behavior_net.to(self.behavior_net.device) # model to cuda\n",
    "        self.target_net.to(self.target_net.device)     # model to cuda\n",
    "        \n",
    "        # train parameters\n",
    "        self.gamma = 0.99 #as written in paper\n",
    "        self.criterion = nn.SmoothL1Loss() # huber loss (error-clipping)\n",
    "        self.optim = torch.optim.RMSprop(self.behavior_net.parameters(), lr=0.00025, alpha=0.95, eps=0.01) #as written in paper in Rainbow\n",
    "        self.k = k\n",
    "        \n",
    "        # train details\n",
    "        self.total_frame = 0\n",
    "        self.total_episode = 0\n",
    "        \n",
    "        self.test_e = 0.05\n",
    "        self.batch_size = 32\n",
    "        self.max_replay = 1000000 # as written in paper\n",
    "        self.min_replay = 50000 # as written in the paper\n",
    "        self.replay_memory = ReplayMemory(self.max_replay)\n",
    "        self.frame_history = FrameHistory(env=self.env, k=self.k)\n",
    "        \n",
    "        self.train_reward_ls = []\n",
    "        self.test_reward_ls = []\n",
    "    \n",
    "    # train-epsilon\n",
    "    @property\n",
    "    def train_e(self):\n",
    "        return np.max([1 - 9.0*1e-07*self.total_frame, 0.1])\n",
    "    \n",
    "    \n",
    "    def reset_episode(self):\n",
    "        # game(episode) begins\n",
    "        self.env.reset()  \n",
    "        self.frame_history.reset() #frame history reset\n",
    "        self.current_episode_frame = 0         # each episode의 frame 수 \n",
    "        self.current_episode_reward = 0        # each episode의 reward 합\n",
    "    \n",
    "    \n",
    "    def choose_action(self, S, is_training=True):\n",
    "        if is_training:\n",
    "            epsilon = self.train_e\n",
    "        else:\n",
    "            epsilon = self.test_e\n",
    "            \n",
    "        # Choose an action by e-greedy\n",
    "        if np.random.rand(1) < epsilon :\n",
    "            a = self.env.action_space.sample()\n",
    "        else:\n",
    "            q_behavior = self.behavior_net(S)\n",
    "            a = torch.argmax(q_behavior).item()\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def run_k_frames(self, is_training=True):\n",
    "        # choose action!\n",
    "        S = self.frame_history.get_state()\n",
    "        a = self.choose_action(S, is_training=is_training)\n",
    "        r_sum = 0\n",
    "        \n",
    "        # repeat the action k-times\n",
    "        for _ in range(self.k):\n",
    "            s_next, r, done, _ = self.env.step(a)\n",
    "            self.frame_history.append_frame(s_next)\n",
    "            r_sum += self.clip_reward(r) # store clipped reward for future training replay(experience)\n",
    "            \n",
    "            # accumulate history\n",
    "            self.current_episode_frame += 1\n",
    "            self.current_episode_reward += r\n",
    "            \n",
    "            # if episode(game) ends, return done(True)\n",
    "            if done: \n",
    "                return done\n",
    "            \n",
    "        # concat last k-frames into a next state S_next\n",
    "        S_next = self.frame_history.get_state()\n",
    "        self.replay_memory.append(S, a, r_sum, S_next, done) # save replay(experience)\n",
    "        return done\n",
    "\n",
    "    \n",
    "    def train(self, max_total_frame):\n",
    "        while self.total_frame < max_total_frame :\n",
    "            # episode starts!\n",
    "            self.reset_episode()\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                done = self.run_k_frames(is_training=True)\n",
    "                \n",
    "                # training when enough replay-memory, every k-frames\n",
    "                if len(self.replay_memory) > self.min_replay:\n",
    "                    self.train_batch(self.batch_size)   \n",
    "                    self.total_frame += self.k\n",
    "                    \n",
    "                # update target_net every 10,000 steps\n",
    "                if self.total_frame%10000== 0 :\n",
    "                    self.update_target()\n",
    "                \n",
    "            # single-episode(game) is now done\n",
    "            # if not enough memory, save more\n",
    "            if len(self.replay_memory) < self.min_replay:\n",
    "                print('Not enough replay yet, expected more than %s, but %s instead'%(self.min_replay, len(self.replay_memory)))\n",
    "            \n",
    "            # if enough memory, print train result\n",
    "            else:\n",
    "                print('Train Episode :%s, Total Frame : %s, Train reward : %s,'%(self.total_episode, self.total_frame, self.current_episode_reward))\n",
    "                self.total_episode += 1\n",
    "                self.train_reward_ls.append(self.current_episode_reward)\n",
    "            \n",
    "                # testing, every 10 episodes when enough replay\n",
    "                if self.total_episode%10 == 0:\n",
    "                    self.test()           \n",
    "\n",
    "    def train_batch(self, batch_size):\n",
    "        # get mini-batch from replay-memory\n",
    "        S, A, R, S_next, D = self.replay_memory.sample(batch_size)\n",
    "        A = self.to_tensor(A, dtype=torch.long).unsqueeze(1)\n",
    "        R = self.to_tensor(R, dtype=torch.float32)\n",
    "        D = self.to_tensor(D, dtype=torch.float32)\n",
    "        \n",
    "        q_behaviors = self.behavior_net(S)                # Q-values for every possible actions\n",
    "        q_behavior = self.select_indices(q_behaviors, A) # select Q-value for given actions\n",
    "        \n",
    "        if self.train_method=='DQN':\n",
    "            q_targets_next = self.target_net(S_next)   # Q-values of every possible actions next state (targetDQN)\n",
    "            q_target_next = q_targets_next.max(1)[0]   # max Q-values of next state\n",
    "            q_target = R + self.gamma*q_target_next*(1-D) \n",
    "        \n",
    "        elif self.train_method=='DoubleDQN':\n",
    "            next_actions = torch.argmax(self.behavior_net(S_next), dim=1, keepdim=True) # choose argmax behavior actions at S_next\n",
    "            \n",
    "            q_targets_next = self.target_net(S_next) # cal Q-values of every possible actions next state (targetDQN)\n",
    "            q_target_next = self.select_indices(q_targets_next, next_actions) # select Q-value for next behavior actions\n",
    "            q_target = R + self.gamma*q_target_next*(1-D)\n",
    "        \n",
    "        # update weights\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.criterion(q_target, q_behavior)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return\n",
    "        \n",
    "    def to_tensor(self, x, dtype=torch.float):\n",
    "        return torch.tensor(x, dtype=dtype, device=self.device)\n",
    "    \n",
    "    def select_indices(self, tensor, indices, dim=1):\n",
    "        return tensor.gather(dim, indices).squeeze(1)\n",
    "    \n",
    "    def clip_reward(self, r):\n",
    "        return np.sign(r)\n",
    "    \n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.behavior_net.state_dict())\n",
    "        return \n",
    "    \n",
    "    def test(self):\n",
    "        self.reset_episode()\n",
    "        \n",
    "        while True:\n",
    "            done = self.run_k_frames(self.test_e) # e-greedy\n",
    "            if done : break\n",
    "                \n",
    "        self.test_reward_ls.append(self.current_episode_reward)\n",
    "        print('※Test※ \\t Frames: %s \\t Score: %s'%(self.current_episode_frame, self.current_episode_reward))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(env, behavior_net, target_net, k=k, train_method='DoubleDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough replay yet, expected more than 50000, but 148 instead\n",
      "Not enough replay yet, expected more than 50000, but 310 instead\n",
      "Not enough replay yet, expected more than 50000, but 539 instead\n",
      "Not enough replay yet, expected more than 50000, but 689 instead\n",
      "Not enough replay yet, expected more than 50000, but 985 instead\n",
      "Not enough replay yet, expected more than 50000, but 1264 instead\n",
      "Not enough replay yet, expected more than 50000, but 1398 instead\n",
      "Not enough replay yet, expected more than 50000, but 1531 instead\n",
      "Not enough replay yet, expected more than 50000, but 1783 instead\n",
      "Not enough replay yet, expected more than 50000, but 1942 instead\n",
      "Not enough replay yet, expected more than 50000, but 2240 instead\n",
      "Not enough replay yet, expected more than 50000, but 2461 instead\n",
      "Not enough replay yet, expected more than 50000, but 2716 instead\n",
      "Not enough replay yet, expected more than 50000, but 2986 instead\n",
      "Not enough replay yet, expected more than 50000, but 3140 instead\n",
      "Not enough replay yet, expected more than 50000, but 3360 instead\n",
      "Not enough replay yet, expected more than 50000, but 3561 instead\n",
      "Not enough replay yet, expected more than 50000, but 3768 instead\n",
      "Not enough replay yet, expected more than 50000, but 3906 instead\n",
      "Not enough replay yet, expected more than 50000, but 4125 instead\n",
      "Not enough replay yet, expected more than 50000, but 4315 instead\n",
      "Not enough replay yet, expected more than 50000, but 4444 instead\n",
      "Not enough replay yet, expected more than 50000, but 4572 instead\n",
      "Not enough replay yet, expected more than 50000, but 4697 instead\n",
      "Not enough replay yet, expected more than 50000, but 4833 instead\n",
      "Not enough replay yet, expected more than 50000, but 4982 instead\n",
      "Not enough replay yet, expected more than 50000, but 5148 instead\n",
      "Not enough replay yet, expected more than 50000, but 5325 instead\n",
      "Not enough replay yet, expected more than 50000, but 5524 instead\n",
      "Not enough replay yet, expected more than 50000, but 5658 instead\n",
      "Not enough replay yet, expected more than 50000, but 5877 instead\n",
      "Not enough replay yet, expected more than 50000, but 6094 instead\n",
      "Not enough replay yet, expected more than 50000, but 6281 instead\n",
      "Not enough replay yet, expected more than 50000, but 6429 instead\n",
      "Not enough replay yet, expected more than 50000, but 6706 instead\n",
      "Not enough replay yet, expected more than 50000, but 6844 instead\n",
      "Not enough replay yet, expected more than 50000, but 7009 instead\n",
      "Not enough replay yet, expected more than 50000, but 7155 instead\n",
      "Not enough replay yet, expected more than 50000, but 7336 instead\n",
      "Not enough replay yet, expected more than 50000, but 7505 instead\n",
      "Not enough replay yet, expected more than 50000, but 7762 instead\n",
      "Not enough replay yet, expected more than 50000, but 7887 instead\n",
      "Not enough replay yet, expected more than 50000, but 8020 instead\n",
      "Not enough replay yet, expected more than 50000, but 8177 instead\n",
      "Not enough replay yet, expected more than 50000, but 8370 instead\n",
      "Not enough replay yet, expected more than 50000, but 8583 instead\n",
      "Not enough replay yet, expected more than 50000, but 8716 instead\n",
      "Not enough replay yet, expected more than 50000, but 8908 instead\n",
      "Not enough replay yet, expected more than 50000, but 9068 instead\n",
      "Not enough replay yet, expected more than 50000, but 9197 instead\n",
      "Not enough replay yet, expected more than 50000, but 9429 instead\n",
      "Not enough replay yet, expected more than 50000, but 9559 instead\n",
      "Not enough replay yet, expected more than 50000, but 9697 instead\n",
      "Not enough replay yet, expected more than 50000, but 9867 instead\n",
      "Not enough replay yet, expected more than 50000, but 10031 instead\n",
      "Not enough replay yet, expected more than 50000, but 10159 instead\n",
      "Not enough replay yet, expected more than 50000, but 10369 instead\n",
      "Not enough replay yet, expected more than 50000, but 10527 instead\n",
      "Not enough replay yet, expected more than 50000, but 10661 instead\n",
      "Not enough replay yet, expected more than 50000, but 10789 instead\n",
      "Not enough replay yet, expected more than 50000, but 10918 instead\n",
      "Not enough replay yet, expected more than 50000, but 11053 instead\n",
      "Not enough replay yet, expected more than 50000, but 11190 instead\n",
      "Not enough replay yet, expected more than 50000, but 11315 instead\n",
      "Not enough replay yet, expected more than 50000, but 11445 instead\n",
      "Not enough replay yet, expected more than 50000, but 11699 instead\n",
      "Not enough replay yet, expected more than 50000, but 11835 instead\n",
      "Not enough replay yet, expected more than 50000, but 12046 instead\n",
      "Not enough replay yet, expected more than 50000, but 12169 instead\n",
      "Not enough replay yet, expected more than 50000, but 12378 instead\n",
      "Not enough replay yet, expected more than 50000, but 12506 instead\n",
      "Not enough replay yet, expected more than 50000, but 12710 instead\n",
      "Not enough replay yet, expected more than 50000, but 12866 instead\n",
      "Not enough replay yet, expected more than 50000, but 13007 instead\n",
      "Not enough replay yet, expected more than 50000, but 13227 instead\n",
      "Not enough replay yet, expected more than 50000, but 13418 instead\n",
      "Not enough replay yet, expected more than 50000, but 13553 instead\n",
      "Not enough replay yet, expected more than 50000, but 13739 instead\n",
      "Not enough replay yet, expected more than 50000, but 13879 instead\n",
      "Not enough replay yet, expected more than 50000, but 14058 instead\n",
      "Not enough replay yet, expected more than 50000, but 14190 instead\n",
      "Not enough replay yet, expected more than 50000, but 14351 instead\n",
      "Not enough replay yet, expected more than 50000, but 14532 instead\n",
      "Not enough replay yet, expected more than 50000, but 14665 instead\n",
      "Not enough replay yet, expected more than 50000, but 14866 instead\n",
      "Not enough replay yet, expected more than 50000, but 15029 instead\n",
      "Not enough replay yet, expected more than 50000, but 15170 instead\n",
      "Not enough replay yet, expected more than 50000, but 15357 instead\n",
      "Not enough replay yet, expected more than 50000, but 15493 instead\n",
      "Not enough replay yet, expected more than 50000, but 15654 instead\n",
      "Not enough replay yet, expected more than 50000, but 15828 instead\n",
      "Not enough replay yet, expected more than 50000, but 16017 instead\n",
      "Not enough replay yet, expected more than 50000, but 16275 instead\n",
      "Not enough replay yet, expected more than 50000, but 16497 instead\n",
      "Not enough replay yet, expected more than 50000, but 16653 instead\n",
      "Not enough replay yet, expected more than 50000, but 16829 instead\n",
      "Not enough replay yet, expected more than 50000, but 17012 instead\n",
      "Not enough replay yet, expected more than 50000, but 17142 instead\n",
      "Not enough replay yet, expected more than 50000, but 17282 instead\n",
      "Not enough replay yet, expected more than 50000, but 17489 instead\n",
      "Not enough replay yet, expected more than 50000, but 17693 instead\n",
      "Not enough replay yet, expected more than 50000, but 17878 instead\n",
      "Not enough replay yet, expected more than 50000, but 18103 instead\n",
      "Not enough replay yet, expected more than 50000, but 18282 instead\n",
      "Not enough replay yet, expected more than 50000, but 18418 instead\n",
      "Not enough replay yet, expected more than 50000, but 18625 instead\n",
      "Not enough replay yet, expected more than 50000, but 18765 instead\n",
      "Not enough replay yet, expected more than 50000, but 18899 instead\n",
      "Not enough replay yet, expected more than 50000, but 19032 instead\n",
      "Not enough replay yet, expected more than 50000, but 19178 instead\n",
      "Not enough replay yet, expected more than 50000, but 19308 instead\n",
      "Not enough replay yet, expected more than 50000, but 19436 instead\n",
      "Not enough replay yet, expected more than 50000, but 19578 instead\n",
      "Not enough replay yet, expected more than 50000, but 19749 instead\n",
      "Not enough replay yet, expected more than 50000, but 19909 instead\n",
      "Not enough replay yet, expected more than 50000, but 20082 instead\n",
      "Not enough replay yet, expected more than 50000, but 20213 instead\n",
      "Not enough replay yet, expected more than 50000, but 20418 instead\n",
      "Not enough replay yet, expected more than 50000, but 20547 instead\n",
      "Not enough replay yet, expected more than 50000, but 20760 instead\n",
      "Not enough replay yet, expected more than 50000, but 20887 instead\n",
      "Not enough replay yet, expected more than 50000, but 21077 instead\n",
      "Not enough replay yet, expected more than 50000, but 21261 instead\n",
      "Not enough replay yet, expected more than 50000, but 21495 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough replay yet, expected more than 50000, but 21700 instead\n",
      "Not enough replay yet, expected more than 50000, but 21835 instead\n",
      "Not enough replay yet, expected more than 50000, but 21973 instead\n",
      "Not enough replay yet, expected more than 50000, but 22145 instead\n",
      "Not enough replay yet, expected more than 50000, but 22283 instead\n",
      "Not enough replay yet, expected more than 50000, but 22421 instead\n",
      "Not enough replay yet, expected more than 50000, but 22555 instead\n",
      "Not enough replay yet, expected more than 50000, but 22700 instead\n",
      "Not enough replay yet, expected more than 50000, but 22864 instead\n",
      "Not enough replay yet, expected more than 50000, but 23037 instead\n",
      "Not enough replay yet, expected more than 50000, but 23175 instead\n",
      "Not enough replay yet, expected more than 50000, but 23331 instead\n",
      "Not enough replay yet, expected more than 50000, but 23489 instead\n",
      "Not enough replay yet, expected more than 50000, but 23620 instead\n",
      "Not enough replay yet, expected more than 50000, but 23752 instead\n",
      "Not enough replay yet, expected more than 50000, but 23892 instead\n",
      "Not enough replay yet, expected more than 50000, but 24115 instead\n",
      "Not enough replay yet, expected more than 50000, but 24240 instead\n",
      "Not enough replay yet, expected more than 50000, but 24379 instead\n",
      "Not enough replay yet, expected more than 50000, but 24553 instead\n",
      "Not enough replay yet, expected more than 50000, but 24697 instead\n",
      "Not enough replay yet, expected more than 50000, but 24916 instead\n",
      "Not enough replay yet, expected more than 50000, but 25052 instead\n",
      "Not enough replay yet, expected more than 50000, but 25265 instead\n",
      "Not enough replay yet, expected more than 50000, but 25525 instead\n",
      "Not enough replay yet, expected more than 50000, but 25659 instead\n"
     ]
    }
   ],
   "source": [
    "max_frame = 10000000\n",
    "fitter.train(max_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fitter.frame_history.get_state()[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
